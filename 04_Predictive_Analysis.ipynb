{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Utilization Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we predict the absolute Utilization (number of cars per hour) as well as the percentage Utilization (percentage of possible charging minutes occupied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\adria\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\google\\protobuf\\descriptor_pool.py:1354\u001b[0m, in \u001b[0;36mDefault\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1350\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1351\u001b[0m   _DEFAULT \u001b[38;5;241m=\u001b[39m DescriptorPool()\n\u001b[1;32m-> 1354\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mDefault\u001b[39m():\n\u001b[0;32m   1355\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _DEFAULT\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras_tuner import HyperModel, RandomSearch\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "charging_set_complete = pd.read_csv(\"data/cleaned_charging_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "charging_set_complete.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_set = pd.read_csv(\"data/cleaned_weather_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weather_set[\"timestamp\"].max(),charging_set_complete[\"connectionTime\"].max())\n",
    "print(weather_set[\"timestamp\"].min(),charging_set_complete[\"connectionTime\"].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the datasets have different time frames, whcih we have to keep in mind for merging later. Unfortunetly this means some data will be lost. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate data into site 1 and site 2\n",
    "charging_set_site1 = charging_set_complete[charging_set_complete[\"siteID\"] == 1]\n",
    "charging_set_site2 = charging_set_complete[charging_set_complete[\"siteID\"] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "charging_set_site1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_duplicates = charging_set_site1.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check space amount of each parking site \n",
    "amount_spaces1 = charging_set_site1[\"spaceID\"].nunique()\n",
    "amount_spaces2 = charging_set_site2[\"spaceID\"].nunique()\n",
    "print(amount_spaces1,amount_spaces2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a dataframe that contains an entry for every hour in between the first and last charging event. This will becime usefull for the prediction later, as the task is to specifically predict hourly utiliaztion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has to be done for Site 1 and 2. The procedure is the same so it will look redundant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "charging_set_site1[\"connectionTime\"] = pd.to_datetime(charging_set_site1[\"connectionTime\"])\n",
    "charging_set_site1[\"disconnectTime\"] = pd.to_datetime(charging_set_site1[\"disconnectTime\"])\n",
    "# sort acsending\n",
    "charging_set_site1 = charging_set_site1.sort_values(by='connectionTime').reset_index(drop=True)\n",
    "\n",
    "# set min and max time \n",
    "min_time = charging_set_site1['connectionTime'].min().floor('H')\n",
    "max_time = charging_set_site1['disconnectTime'].max().ceil('H')\n",
    "\n",
    "# create df for every hour \n",
    "hours_df = pd.DataFrame({'hour': pd.date_range(start=min_time, end=max_time, freq='H')})\n",
    "hours_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "charging_set_site2[\"connectionTime\"] = pd.to_datetime(charging_set_site2[\"connectionTime\"])\n",
    "charging_set_site2[\"disconnectTime\"] = pd.to_datetime(charging_set_site2[\"disconnectTime\"])\n",
    "\n",
    "# sort ascending\n",
    "charging_set_site2 = charging_set_site2.sort_values(by='connectionTime').reset_index(drop=True)\n",
    "\n",
    "# set min and max time \n",
    "min_time = charging_set_site2['connectionTime'].min().floor('H')\n",
    "max_time = charging_set_site2['disconnectTime'].max().ceil('H')\n",
    "\n",
    "# create df for every hour \n",
    "hours_df2 = pd.DataFrame({'hour': pd.date_range(start=min_time, end=max_time, freq='H')})\n",
    "hours_df2.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are different timeframes of data available for both sites. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n"
     ]
    }
   ],
   "source": [
    "# check for duplicates to make sure df creation was succesful \n",
    "print(hours_df.duplicated().sum(), hours_df2.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1412 0\n"
     ]
    }
   ],
   "source": [
    "# check for duplicates in charging sets\n",
    "print(charging_set_site1.duplicated().sum(), charging_set_site2.duplicated().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# drop duplicates we foundin site 1\n",
    "charging_set_site1 = charging_set_site1.drop_duplicates()\n",
    "charging_set_site1.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to create our target variables that we want to predict later. For this we create 2 values: \n",
    "- connectedCars: the number of active charging sessions for each other\n",
    "- utilization: the actual number of minutes that was charged for each hour \n",
    "\n",
    "The utiliaztion column effectivly sums up the active charging minutes for each hour and divides that by the maximum number of minutes that could be charged. In case of sitze 1 for example that would mean 60x52 = 3120 are the max charging miinutes for this site per hour. If we divide the minutes of active sessions by this value, we get the precise percentage of the charging site utilization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculation for Site 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hours_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 30\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m utilization, connected_cars\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# compute utilization column\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m hours_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutilization\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mhours_df\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhour\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m h: calculate_utilization(h, charging_set_site1, total_spaces)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     32\u001b[0m )\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# compute connectedCars column\u001b[39;00m\n\u001b[0;32m     35\u001b[0m hours_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconnectedCars\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m hours_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhour\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m h: calculate_utilization(h, charging_set_site1, total_spaces)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     37\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'hours_df' is not defined"
     ]
    }
   ],
   "source": [
    "# number of spaces (total available spaces site 1)\n",
    "total_spaces = 52\n",
    "\n",
    "# function to calculate active session minutes for each hour\n",
    "def calculate_utilization(hour, charging_set_site1, total_spaces):\n",
    "    start_of_hour = hour\n",
    "    end_of_hour = hour + pd.Timedelta(hours=1)\n",
    "    \n",
    "    # filter sessions active during this hour\n",
    "    active_sessions = charging_set_site1[\n",
    "        (charging_set_site1['connectionTime'] < end_of_hour) & (charging_set_site1['disconnectTime'] > start_of_hour)\n",
    "    ]\n",
    "    \n",
    "    # calculate active minutes for each session\n",
    "    active_minutes = 0\n",
    "    for _, session in active_sessions.iterrows():\n",
    "        session_start = max(session['connectionTime'], start_of_hour)\n",
    "        session_end = min(session['disconnectTime'], end_of_hour)\n",
    "        active_minutes += (session_end - session_start).total_seconds() / 60  # Convert to minutes\n",
    "    \n",
    "    # count number of cars\n",
    "    connected_cars = len(active_sessions)\n",
    "    # utilization as a percentage\n",
    "    possible_minutes = total_spaces * 60  # 60 minutes per hour per space\n",
    "    utilization = active_minutes / possible_minutes if possible_minutes > 0 else 0\n",
    "    return utilization, connected_cars\n",
    "\n",
    "\n",
    "# compute utilization column\n",
    "hours_df['utilization'] = hours_df['hour'].apply(\n",
    "    lambda h: calculate_utilization(h, charging_set_site1, total_spaces)[0]\n",
    ")\n",
    "\n",
    "# compute connectedCars column\n",
    "hours_df['connectedCars'] = hours_df['hour'].apply(\n",
    "    lambda h: calculate_utilization(h, charging_set_site1, total_spaces)[1]\n",
    ")\n",
    "\n",
    "# display results\n",
    "hours_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hours_df2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 30\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m utilization, connected_cars\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# compute utilization column\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m hours_df2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutilization\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mhours_df2\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhour\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m h: calculate_utilization(h, charging_set_site2, total_spaces)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     32\u001b[0m )\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# compute connectedCars column\u001b[39;00m\n\u001b[0;32m     35\u001b[0m hours_df2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconnectedCars\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m hours_df2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhour\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m h: calculate_utilization(h, charging_set_site2, total_spaces)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     37\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'hours_df2' is not defined"
     ]
    }
   ],
   "source": [
    "# number of spaces (total available spaces site 1)\n",
    "total_spaces = 54\n",
    "\n",
    "# function to calculate active session minutes for each hour\n",
    "def calculate_utilization(hour, charging_set_site2, total_spaces):\n",
    "    start_of_hour = hour\n",
    "    end_of_hour = hour + pd.Timedelta(hours=1)\n",
    "    \n",
    "    # filter sessions active during this hour\n",
    "    active_sessions = charging_set_site2[\n",
    "        (charging_set_site2['connectionTime'] < end_of_hour) & (charging_set_site2['disconnectTime'] > start_of_hour)\n",
    "    ]\n",
    "    \n",
    "    # calculate active minutes for each session\n",
    "    active_minutes = 0\n",
    "    for _, session in active_sessions.iterrows():\n",
    "        session_start = max(session['connectionTime'], start_of_hour)\n",
    "        session_end = min(session['disconnectTime'], end_of_hour)\n",
    "        active_minutes += (session_end - session_start).total_seconds() / 60  # Convert to minutes\n",
    "    \n",
    "    # count number of cars\n",
    "    connected_cars = len(active_sessions)\n",
    "    # utilization as a percentage\n",
    "    possible_minutes = total_spaces * 60  # 60 minutes per hour per space\n",
    "    utilization = active_minutes / possible_minutes if possible_minutes > 0 else 0\n",
    "    return utilization, connected_cars\n",
    "\n",
    "\n",
    "# compute utilization column\n",
    "hours_df2['utilization'] = hours_df2['hour'].apply(\n",
    "    lambda h: calculate_utilization(h, charging_set_site2, total_spaces)[0]\n",
    ")\n",
    "\n",
    "# compute connectedCars column\n",
    "hours_df2['connectedCars'] = hours_df2['hour'].apply(\n",
    "    lambda h: calculate_utilization(h, charging_set_site2, total_spaces)[1]\n",
    ")\n",
    "\n",
    "# display results\n",
    "hours_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hours_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hours_df2.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge the datasets on the timestamp columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge Target Variable and Weather Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform column to the same format\n",
    "hours_df[\"hour\"] = hours_df[\"hour\"].dt.tz_localize(None)\n",
    "hours_df2[\"hour\"] = hours_df2[\"hour\"].dt.tz_localize(None)\n",
    "\n",
    "# convert the \"timestamp\" column in the weather data to datetime\n",
    "weather_set[\"timestamp\"] = pd.to_datetime(weather_set[\"timestamp\"])\n",
    "\n",
    "# round the weather data timestamps to the nearest hour\n",
    "weather_set[\"timestamp\"] = weather_set[\"timestamp\"].dt.floor(\"H\")\n",
    "\n",
    "# merge the datasets based on the \"timestamp\" column\n",
    "merged_data_site1 = pd.merge(hours_df, weather_set, left_on=\"hour\", right_on=\"timestamp\", how=\"left\")\n",
    "merged_data_site2 = pd.merge(hours_df2, weather_set, left_on=\"hour\", right_on=\"timestamp\", how=\"left\")\n",
    "# drop the redundant \"timestamp\" column from the weather data after merging\n",
    "merged_data_site1 = merged_data_site1.drop(columns=[\"timestamp\"])\n",
    "merged_data_site2 = merged_data_site2.drop(columns=[\"timestamp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data_site1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data_site2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "???? drunter weil 2 mal Site 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check dataset range\n",
    "print(merged_data_site1[\"hour\"].max(),merged_data_site1[\"hour\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check NaN values \n",
    "print(merged_data_site1.isna().sum(),merged_data_site2.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop NaN values\n",
    "merged_data_site1 = merged_data_site1.dropna()\n",
    "merged_data_site2 = merged_data_site2.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if NaN values were dropped\n",
    "print(merged_data_site1.isna().sum(),merged_data_site2.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check max date now\n",
    "print(merged_data_site1[\"hour\"].max(),merged_data_site2[\"hour\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chec for duplicates\n",
    "print(merged_data_site1.duplicated().sum(),merged_data_site2.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data_site1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data_site2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = \"2020-08-02\"\n",
    "end_date = \"2020-10-31\"\n",
    "\n",
    "filtered_data = merged_data_site1.loc[\n",
    "    (merged_data_site1['hour'] >= pd.to_datetime(start_date)) &\n",
    "    (merged_data_site1['hour'] <= pd.to_datetime(end_date))\n",
    "]\n",
    "filtered_data2 = merged_data_site2.loc[\n",
    "    (merged_data_site2['hour'] >= pd.to_datetime(start_date)) &\n",
    "    (merged_data_site2['hour'] <= pd.to_datetime(end_date))\n",
    "]\n",
    "print(len(filtered_data),len(filtered_data2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_connected_cars_count = filtered_data[filtered_data['connectedCars'] == 0].shape[0]\n",
    "zero_connected_cars_count2 = filtered_data2[filtered_data2['connectedCars'] == 0].shape[0]\n",
    "print(zero_connected_cars_count,zero_connected_cars_count2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in the descriptive analysis (Section 2) there is a gap of data between august 2020 and Oktober 2020. This will most likely have a negativ impact on our model performance because as seen above those values are almost all 0. Therefore we are dropping them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data_site1 = merged_data_site1.loc[\n",
    "    ~((merged_data_site1['hour'] >= pd.to_datetime(start_date)) &\n",
    "      (merged_data_site1['hour'] <= pd.to_datetime(end_date)))\n",
    "]\n",
    "merged_data_site2 = merged_data_site2.loc[\n",
    "    ~((merged_data_site2['hour'] >= pd.to_datetime(start_date)) &\n",
    "      (merged_data_site2['hour'] <= pd.to_datetime(end_date)))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data_site1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data_site2.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering for Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the features to site 1 and site 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create season column and weekend/weekday column and covvid column\n",
    "merged_data_site1[\"hour\"] = pd.to_datetime(merged_data_site1[\"hour\"])\n",
    "# add covid column\n",
    "merged_data_site1[\"covid\"] = merged_data_site1[\"hour\"].apply(lambda x: 0 if x < pd.Timestamp(\"2020-03-01\") else 1)\n",
    "\n",
    "merged_data_site1[\"weekday\"] = merged_data_site1[\"hour\"].apply(lambda x: x.weekday())\n",
    " \n",
    "# function to map months to seasons\n",
    "def get_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return \"Winter\"\n",
    "    elif month in [3, 4, 5]:\n",
    "        return \"Spring\"\n",
    "    elif month in [6, 7, 8]:\n",
    "        return \"Summer\"\n",
    "    elif month in [9, 10,11]:\n",
    "        return \"Fall\"\n",
    "    return np.nan\n",
    "\n",
    "# apply the season mapping\n",
    "merged_data_site1[\"Season\"] = merged_data_site1[\"date_month\"].apply(get_season)\n",
    "\n",
    "# add weekday/weekend column \n",
    "merged_data_site1[\"Weekday/Weekend\"] = merged_data_site1[\"hour\"].apply(lambda x: 1 if x.weekday()>=5 else 0)\n",
    "# mapping seasons to numeric values\n",
    "season_mapping = {\"Winter\": 0, \"Spring\": 1, \"Summer\": 2, \"Fall\": 3}\n",
    "merged_data_site1[\"Season\"] = merged_data_site1[\"Season\"].map(season_mapping)\n",
    "\n",
    "\n",
    "# create df column for time of day\n",
    "def assign_category(hour):\n",
    "    if hour >= 23 or hour < 6:\n",
    "        return 0  # Night\n",
    "    elif 6 <= hour < 12:\n",
    "        return 1  # Morning\n",
    "    elif 12 <= hour < 18:\n",
    "        return 2  # Afternoon\n",
    "    else:\n",
    "        return 3  # Evening\n",
    "\n",
    "# apply the function to create a new column\n",
    "merged_data_site1[\"time_of_day\"] = merged_data_site1[\"hour\"].dt.hour.apply(assign_category)\n",
    "\n",
    "# create  precise hour of day column\n",
    "merged_data_site1[\"hour_of_day\"] = merged_data_site1[\"hour\"].dt.hour\n",
    "\n",
    "# resulting DataFrame\n",
    "merged_data_site1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create season column and weekend/weekday column and covvid column\n",
    "merged_data_site2[\"hour\"] = pd.to_datetime(merged_data_site2[\"hour\"])\n",
    "# add covid column\n",
    "merged_data_site2[\"covid\"] = merged_data_site2[\"hour\"].apply(lambda x: 0 if x < pd.Timestamp(\"2020-03-01\") else 1)\n",
    "\n",
    "merged_data_site2[\"weekday\"] = merged_data_site2[\"hour\"].apply(lambda x: x.weekday())\n",
    " \n",
    "# function to map months to seasons\n",
    "def get_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return \"Winter\"\n",
    "    elif month in [3, 4, 5]:\n",
    "        return \"Spring\"\n",
    "    elif month in [6, 7, 8]:\n",
    "        return \"Summer\"\n",
    "    elif month in [9, 10,11]:\n",
    "        return \"Fall\"\n",
    "    return np.nan\n",
    "\n",
    "# apply the season mapping\n",
    "merged_data_site2[\"Season\"] = merged_data_site2[\"date_month\"].apply(get_season)\n",
    "\n",
    "# add weekday/weekend column \n",
    "merged_data_site2[\"Weekday/Weekend\"] = merged_data_site2[\"hour\"].apply(lambda x: 1 if x.weekday()>=5 else 0)\n",
    "# mapping seasons to numeric values\n",
    "season_mapping = {\"Winter\": 0, \"Spring\": 1, \"Summer\": 2, \"Fall\": 3}\n",
    "merged_data_site2[\"Season\"] = merged_data_site2[\"Season\"].map(season_mapping)\n",
    "\n",
    "\n",
    "# create df column for time of day\n",
    "def assign_category(hour):\n",
    "    if hour >= 23 or hour < 6:\n",
    "        return 0  # Night\n",
    "    elif 6 <= hour < 12:\n",
    "        return 1  # Morning\n",
    "    elif 12 <= hour < 18:\n",
    "        return 2  # Afternoon\n",
    "    else:\n",
    "        return 3  # Evening\n",
    "\n",
    "# apply the function to create a new column\n",
    "merged_data_site2[\"time_of_day\"] = merged_data_site2[\"hour\"].dt.hour.apply(assign_category)\n",
    "\n",
    "# create  precise hour of day column\n",
    "merged_data_site2[\"hour_of_day\"] = merged_data_site2[\"hour\"].dt.hour\n",
    "# resulting DataFrame\n",
    "merged_data_site2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Site 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'merged_data_site1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Compute correlation matrix\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m correlation_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mmerged_data_site1\u001b[49m\u001b[38;5;241m.\u001b[39mcorr()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Correlation with target feature\u001b[39;00m\n\u001b[0;32m      5\u001b[0m target_correlation \u001b[38;5;241m=\u001b[39m correlation_matrix[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconnectedCars\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msort_values(ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'merged_data_site1' is not defined"
     ]
    }
   ],
   "source": [
    "# compute correlation matrix\n",
    "correlation_matrix = merged_data_site1.corr()\n",
    "\n",
    "# correlation with target feature\n",
    "target_correlation = correlation_matrix['connectedCars'].sort_values(ascending=False)\n",
    "print(\"Correlation with connectedCars:\")\n",
    "print(target_correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute correlation and display correlation for newly selected features\n",
    "predictionDF_site1 = merged_data_site1[[\"connectedCars\", \"utilization\",\"time_of_day\",\"windspeed\",\"temperature\",\"felt_temperature\",\"date_month\",\"weekday\",\"Weekday/Weekend\",\"covid\",\"hour_of_day\"]]\n",
    "correlation_matrix = predictionDF_site1.corr()\n",
    "# plot heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Site 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute correlation matrix\n",
    "correlation_matrix = merged_data_site2.corr()\n",
    "\n",
    "\n",
    "# correlation with target feature\n",
    "target_correlation = correlation_matrix['connectedCars'].sort_values(ascending=False)\n",
    "print(\"Correlation with connectedCars:\")\n",
    "print(target_correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute correlation and display correlation for newly selected features\n",
    "predictionDF_site2 = merged_data_site2[[\"connectedCars\", \"utilization\",\"time_of_day\",\"windspeed\",\"temperature\",\"felt_temperature\",\"Season\",\"date_month\",\"weekday\",\"Weekday/Weekend\",\"covid\",\"hour_of_day\"]]\n",
    "correlation_matrix = predictionDF_site2.corr()\n",
    "# plot heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ConnectedCars Prediction for Site 1 with Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = predictionDF_site1.copy()\n",
    "X = predictionDF_site1.drop([\"connectedCars\",\"utilization\",\"temperature\",\"time_of_day\"],axis = 1)\n",
    "y = predictionDF_site1[[\"connectedCars\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.dropna()\n",
    "y = y.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# standardize the feature values\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise model\n",
    "model = Sequential(\n",
    "    [Dense(160, activation=\"relu\", input_shape=[X_train.shape[1]]),\n",
    "    Dense(64, activation=\"relu\"),\n",
    "    Dense(32, activation=\"relu\"),\n",
    "     Dense(32, activation=\"relu\"),\n",
    "      Dense(32, activation=\"relu\"),\n",
    "       Dense(32, activation=\"relu\"),\n",
    "       Dense(32, activation=\"relu\"),\n",
    "       Dense(32, activation=\"relu\"),\n",
    "     Dense(1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "model.compile(optimizer=\"adam\", loss = \"mean_squared_error\",\n",
    "             metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "epochs = 65\n",
    "\n",
    "history = model.fit(X_train, y_train.values,\n",
    "                   epochs=epochs, validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "r2 = r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating the model on test data\n",
    "loss, mae = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss}, Test MAE: {mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connected Cars Prediction for Site 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X = predictionDF_site2[[\"hour_of_day\",\"covid\",\"Weekday/Weekend\",\"weekday\",\"Season\",\"temperature\",\"windspeed\"]]\n",
    "y = predictionDF_site2[[\"connectedCars\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize the feature values\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise model\n",
    "model = Sequential(\n",
    "    [Dense(160, activation=\"relu\", input_shape=[X_train.shape[1]]),\n",
    "    Dense(64, activation=\"relu\"),\n",
    "    Dense(32, activation=\"relu\"),\n",
    "     Dense(32, activation=\"relu\"),\n",
    "      Dense(32, activation=\"relu\"),\n",
    "       Dense(32, activation=\"relu\"),\n",
    "       Dense(32, activation=\"relu\"),\n",
    "       Dense(32, activation=\"relu\"),\n",
    "     Dense(1)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
